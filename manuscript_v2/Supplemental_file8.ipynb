{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting between BioC xml and BioC json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script converts a BioC collection to BioC json or vice versa (adapted from the BioC-JSON tools from the NLM/NCBI BioNLP Research Group: bioc_json.py, downloaded 20200312 from https://github.com/ncbi-nlp/BioC-JSON).\n",
    "\n",
    "Dependencies: PyBioC (available from https://github.com/2mh/PyBioC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import bioc\n",
    "from bioc import biocxml\n",
    "\n",
    "class BioC2JSON:\n",
    "    def node(this, node):\n",
    "        json_node = {'refid': node.refid, 'role': node.role}\n",
    "        return json_node\n",
    "    \n",
    "    def relation(this, rel):\n",
    "        json_rel = {}\n",
    "        json_rel['id'] = rel.id\n",
    "        json_rel['infons'] = rel.infons\n",
    "        json_rel['nodes'] = [this.node(n) for n in rel.nodes] \n",
    "        return json_rel\n",
    "\n",
    "    def location(this, loc):\n",
    "        json_loc = {'offset': int(loc.offset), 'length': int(loc.length)}\n",
    "        return json_loc\n",
    "\n",
    "    def annotation(this, note):\n",
    "        json_note = {}\n",
    "        json_note['id'] = note.id\n",
    "        json_note['infons'] = note.infons\n",
    "        json_note['text'] = note.text\n",
    "        json_note['locations'] = [this.location(l)\n",
    "                                  for l in note.locations] \n",
    "        return json_note\n",
    "    \n",
    "    def sentence(this, sent):\n",
    "        json_sent = {}\n",
    "        json_sent['infons'] = sent.infons\n",
    "        json_sent['offset'] = int(sent.offset)\n",
    "        json_sent['text'] = sent.text\n",
    "        json_sent['annotations'] = [this.annotation(a)\n",
    "                                    for a in sent.annotations]\n",
    "        json_sent['relations'] = [this.relation(r)\n",
    "                                  for r in sent.relations] \n",
    "        return json_sent\n",
    "\n",
    "    def passage(this, psg):\n",
    "        json_psg = {}\n",
    "        json_psg['infons'] = psg.infons\n",
    "        json_psg['offset'] = int(psg.offset)\n",
    "        json_psg['text'] =  psg.text\n",
    "        json_psg['text'] =  psg.text if psg.text else \"\"\n",
    "        json_psg['sentences'] = [this.sentence(s)\n",
    "                                 for s in psg.sentences] \n",
    "        json_psg['annotations'] = [this.annotation(a)\n",
    "                                   for a in psg.annotations]\n",
    "        json_psg['relations'] = [this.relation(r)\n",
    "                                 for r in psg.relations] \n",
    "        return json_psg\n",
    "\n",
    "    def document(this, doc):\n",
    "        json_doc = {}\n",
    "        json_doc['id'] = doc.id\n",
    "        json_doc['infons'] = doc.infons\n",
    "        json_doc['passages'] = [this.passage(p)\n",
    "                                for p in doc.passages]\n",
    "        json_doc['relations'] = [this.relation(r)\n",
    "                                 for r in doc.relations] \n",
    "        return json_doc\n",
    "\n",
    "    def collection(this, collection):\n",
    "        json_collection = {}\n",
    "        json_collection['source'] = collection.source\n",
    "        json_collection['date'] = collection.date\n",
    "        json_collection['key'] = collection.key\n",
    "        json_collection['infons'] = collection.infons\n",
    "        json_collection['documents'] = [this.document(d)\n",
    "                                        for d in collection.documents] \n",
    "        return json_collection\n",
    "\n",
    "class JSON2BioC:\n",
    "\n",
    "    def node(this, json_node):\n",
    "        node = bioc.BioCNode()\n",
    "        node.refid = json_node['refid']\n",
    "        node.role = json_node['role']\n",
    "        return node\n",
    "\n",
    "    def relation(this, json_rel):\n",
    "        rel = bioc.BioCRelation()\n",
    "        rel.id = json_rel['id']\n",
    "        rel.infons = json_rel['infons']\n",
    "        rel.nodes = [this.node(n) for n in json_rel['nodes']] \n",
    "        return rel\n",
    "\n",
    "    def location(this, json_loc):\n",
    "        loc = bioc.BioCLocation()\n",
    "        loc.offset = str(json_loc['offset'])\n",
    "        loc.length = str(json_loc['length'])\n",
    "        return loc\n",
    "\n",
    "    def annotation(this, json_note):\n",
    "        note = bioc.BioCAnnotation()\n",
    "        note.id = json_note['id']\n",
    "        note.infons = json_note['infons']\n",
    "        note.text = json_note['text']\n",
    "        note.locations = [this.location(l)\n",
    "                          for l in json_note['locations']] \n",
    "        return note\n",
    "    \n",
    "    def sentence(this, json_sent):\n",
    "        sent = bioc.BioCSentence()\n",
    "        sent.infons = json_sent['infons']\n",
    "        sent.offset = str(json_sent['offset'])\n",
    "        sent.text = json_sent['text']\n",
    "        sent.annotations = [this.annotation(a)\n",
    "                            for a in json_sent['annotations']]\n",
    "        sent.relations = [this.relation(r)\n",
    "                          for r in json_sent['relations']]\n",
    "        return sent\n",
    "\n",
    "    def passage(this, json_psg):\n",
    "        psg = bioc.BioCPassage()\n",
    "        psg.infons = json_psg['infons']\n",
    "        psg.offset = str(json_psg['offset'])\n",
    "        psg.text = json_psg.get('text')\n",
    "        psg.sentences = [this.sentence(s)\n",
    "                         for s in json_psg['sentences']]\n",
    "        psg.annotations = [this.annotation(a)\n",
    "                           for a in json_psg['annotations']]\n",
    "        psg.relations = [this.relation(r)\n",
    "                         for r in json_psg['relations']]\n",
    "        return psg\n",
    "\n",
    "    def document(this, json_doc):\n",
    "        doc = bioc.BioCDocument()\n",
    "        doc.id = json_doc['id']\n",
    "        doc.infons = json_doc['infons']\n",
    "        doc.passages = [this.passage(p)\n",
    "                        for p in json_doc['passages']]\n",
    "        doc.relations = [this.relation(r)\n",
    "                         for r in json_doc['relations']]\n",
    "        return doc\n",
    "\n",
    "    def collection(this, json_collection):\n",
    "        collection = bioc.BioCCollection()\n",
    "        collection.source = json_collection['source']\n",
    "        collection.date = json_collection['date'] \n",
    "        collection.key = json_collection['key']\n",
    "        collection.infons = json_collection['infons']\n",
    "        collection.documents = [this.document(d)\n",
    "                                for d in json_collection['documents']]\n",
    "        return collection\n",
    "\n",
    "option = \"-j\" #change to -b when converting from json to xml\n",
    "in_file = \"../../../data/gold_standard_xml/goldstandard2_20220203.xml\"#'path to input file'\n",
    "out_file = \"../../../results/supple.json\"#'path to output file'\n",
    "\n",
    "if option == '-j': #converts xml to json\n",
    "    with open(in_file,\"r\") as reader:\n",
    "        collection = bioc.load(reader)\n",
    "        bioc2json = BioC2JSON()\n",
    "        bioc_json = bioc2json.collection(collection)\n",
    "        with open(out_file, 'w') as f:\n",
    "            json.dump(bioc_json, f, indent=2)\n",
    "            print(file=f)\n",
    "\n",
    "elif option == '-b': #converts json to xml\n",
    "        bioc_json = None\n",
    "        with open(in_file) as f:\n",
    "            bioc_json = json.load(f)\n",
    "\n",
    "        # print json.dumps(bioc_json, indent=2)\n",
    "\n",
    "        json2bioc = JSON2BioC()\n",
    "        bioc_collection = json2bioc.collection(bioc_json)\n",
    "        \n",
    "        writer = bioc.BioCWriter(out_file, bioc_collection)\n",
    "        writer.write()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_hyphens_lowercase(list_):\n",
    "    '''change terms from list to lowercase and remove hyphens\n",
    "    '''\n",
    "    new_set = set()\n",
    "    for term in list_:\n",
    "        new_set.update([term.lower().replace(\"-\",\" \")])\n",
    "    return sorted(list(new_set))\n",
    "\n",
    "def create_variant_terms_dis(list_):\n",
    "    '''\n",
    "    used to create a new variant list if keyword exists in the term\n",
    "    these terms are ok unless severe acute respiratory syndrome is within the list\n",
    "    (disease dictionary)\n",
    "    '''\n",
    "    \n",
    "    new_set = set()\n",
    "    variants = [\"disease\", \"disorder\", \"syndrome\", \"pneumonia\", \"infection\"]\n",
    "    for term in list_:\n",
    "        term = term.lower()\n",
    "        new_set.update([term])\n",
    "        \n",
    "        main_var = \"\"\n",
    "        keyword_in_term = False\n",
    "        new_var_list = []\n",
    "        \n",
    "        for var in variants:\n",
    "            if var in term:\n",
    "                keyword_in_term = True\n",
    "                main_var = var\n",
    "                new_var_list = [v for v in variants if v!=var]\n",
    "                break\n",
    "        \n",
    "        if keyword_in_term == True:\n",
    "            for var2 in new_var_list:\n",
    "                new_set.update([term.replace(main_var, var2)])\n",
    "                \n",
    "    return sorted(new_set)\n",
    "\n",
    "            \n",
    "def interchange_corona(list_):\n",
    "    '''\n",
    "    interchange corona and coronavirus terms \n",
    "    '''\n",
    "    new_set = set()\n",
    "    for term in list_:\n",
    "        term = term.lower()\n",
    "        new_set.update([term])\n",
    "        if \"corona virus\" in term:\n",
    "            continue\n",
    "        elif \"coronavirus\" in term:\n",
    "            if not \"syndrome coronavirus\" in term:\n",
    "                new_set.update([term.replace(\"coronavirus\", \"corona\")])\n",
    "        else:\n",
    "            new_set.update([re.sub(\"\\bcorona\\b\", \"coronavirus\", term)])\n",
    "            \n",
    "    return sorted(new_set)\n",
    "\n",
    "\n",
    "def interchange_wuhan_hubei(list_):\n",
    "    '''\n",
    "    interchanging wuhan and hubei\n",
    "    '''\n",
    "    new_set = set()\n",
    "    \n",
    "    for term in list_:\n",
    "        term = term.lower()\n",
    "        \n",
    "        new_set.update([term])\n",
    "        \n",
    "        if \"wuhan\" in term:\n",
    "            new_set.update([term.replace(\"wuhan\", \"hubei\")])\n",
    "        elif \"hubei\" in term:\n",
    "            new_set.update([term.replace(\"hubei\", \"wuhan\")])\n",
    "            \n",
    "    return sorted(new_set)\n",
    "\n",
    "def check_duplicate_words_from_single_term(term):\n",
    "    words = term.split()\n",
    "    return len(words) > len(set(words))\n",
    "\n",
    "def check_if_both_new_novel(term):\n",
    "    return \"new\" in term and \"novel\" in term\n",
    "\n",
    "def remove_duplicates(list_):\n",
    "    '''\n",
    "    remove duplicate terms and remove \"new\"/\"novel\" multiple occurances\n",
    "    '''\n",
    "    new_set = set()\n",
    "    \n",
    "    for term in list_:\n",
    "        if check_duplicate_words_from_single_term(term):\n",
    "            continue\n",
    "        elif check_if_both_new_novel(term):\n",
    "            continue\n",
    "        else:\n",
    "            new_set.update([term])\n",
    "    \n",
    "    return sorted(new_set) \n",
    "\n",
    "def _2019_var_virus(list_):\n",
    "    '''\n",
    "    create variants with 2019 and new/novel (virus dictionary)\n",
    "    '''\n",
    "    variants_prefix = ['2019novel', '2019new', '2019 novel', '2019 new']\n",
    "    \n",
    "    new_set = set()\n",
    "    \n",
    "    for term in list_:\n",
    "        new_set.update([term])\n",
    "        if \"2019\" not in term and \"19\" not in term:\n",
    "            # add 2019 as prefix and suffix\n",
    "            new_set.update([term + \" 2019\"])\n",
    "            new_set.update([\"2019 \" + term])            \n",
    "            \n",
    "            for var in variants_prefix:\n",
    "                new_set.update([var + \" \" + term])\n",
    "\n",
    "        \n",
    "    new_set_2 = set()\n",
    "    #check for duplicates\n",
    "    for term in new_set:\n",
    "        if check_duplicate_words_from_single_term(term):\n",
    "            continue\n",
    "        elif check_if_both_new_novel(term):\n",
    "            continue\n",
    "        elif re.compile(r'\\b2019new\\b').search(term) and re.compile(r'\\bnew\\b').search(term):\n",
    "            continue\n",
    "        elif re.compile(r'\\b2019novel\\b').search(term) and re.compile(r'\\bnovel\\b').search(term):\n",
    "            continue\n",
    "        else:\n",
    "            new_set_2.update([term])\n",
    "        \n",
    "    return sorted(new_set_2)\n",
    "\n",
    "def create_dis_variants_from_virus_cv(cv_terms, virus_terms):\n",
    "    '''\n",
    "    for the disease dictionary, create disease variants from virus terms\n",
    "    '''\n",
    "    disease_from_virus = set()\n",
    "    for virus in virus_terms:\n",
    "        for term in cv_terms:\n",
    "            if \"[virus name or Wuhan or Hubei]\" in term:\n",
    "                disease_from_virus.update([term.replace(\"[virus name or Wuhan or Hubei]\",virus)])\n",
    "                disease_from_virus.update([term.replace(\"[virus name or Wuhan or Hubei]\",\"wuhan\")])\n",
    "                disease_from_virus.update([term.replace(\"[virus name or Wuhan or Hubei]\",\"hubei\")])\n",
    "\n",
    "            elif \"[virus name]\" in term:\n",
    "                disease_from_virus.update([term.replace(\"[virus name]\",virus)])\n",
    "\n",
    "    disease_from_virus_lower = remove_hyphens_lowercase(disease_from_virus)\n",
    "    disease_from_virus_lower_nodup = remove_duplicates(disease_from_virus_lower)\n",
    "    \n",
    "    return sorted(disease_from_virus_lower_nodup)\n",
    "\n",
    "def read_input_file(filename):\n",
    "    '''read file and create a list\n",
    "    '''\n",
    "    new_set = set()\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            line=line.strip()\n",
    "            new_set.update([line])\n",
    "    \n",
    "    return sorted(new_set)\n",
    "\n",
    "def write_output_file(filename, list_):\n",
    "    '''\n",
    "    write list into output file\n",
    "    '''\n",
    "    with open(filename, 'w') as f:\n",
    "        for term in sorted(list_):\n",
    "            f.write(term + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## virus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file virus\n",
    "v_in = \"virus/input/file.txt\"\n",
    "# output file virus\n",
    "v_out = \"virus/output/file.txt\"\n",
    "\n",
    "virus_terms = read_input_file(v_in)\n",
    "\n",
    "# remove hyphens and lowercase all terms\n",
    "virus_lowercase = remove_hyphens_lowercase(virus_terms)\n",
    "\n",
    "# add 2019 and new/novel variants\n",
    "virus_lowercase_2019 = _2019_var_virus(virus_lowercase)\n",
    "\n",
    "# interchange corona and coronavirus terms\n",
    "virus_lowercase_2019_corona = interchange_corona(virus_lowercase_2019)\n",
    "\n",
    "# interchange wuhan and hubei terms\n",
    "virus_lowercase_2019_corona_wuhan = interchange_wuhan_hubei(virus_lowercase_2019_corona)\n",
    "\n",
    "# remove duplicate words and terms with both new and novel\n",
    "virus_lowercase_2019_corona_wuhan_nodup = remove_duplicates(virus_lowercase_2019_corona_wuhan)\n",
    "\n",
    "# write file\n",
    "write_output_file(v_out, virus_lowercase_2019_corona_wuhan_nodup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## disease dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file disease\n",
    "dis_in = \"disease/input/file.txt\"\n",
    "# output file disease\n",
    "dis_out = \"disease/output/file.txt\"\n",
    "\n",
    "# file with terms to convert virus terms into disease\n",
    "cv_in = \"disease/input/cv_file.md\"\n",
    "\n",
    "disease_terms = read_input_file(dis_in)\n",
    "cv_terms = read_input_file(cv_in)\n",
    "\n",
    "# remove hyphens and lowercase all terms\n",
    "dis_lowercase = remove_hyphens_lowercase(disease_terms)\n",
    "\n",
    "# create disease variants from virus terms (get the virus terms from the virus dictionary or load from file)\n",
    "dis_v_variants = create_dis_variants_from_virus_cv(cv_terms, virus_lowercase_2019_corona_wuhan_nodup)\n",
    "\n",
    "# merge lists\n",
    "dis_merged = set.union(set(dis_lowercase), set(dis_v_variants))\n",
    "\n",
    "# interchange corona and coronavirus terms\n",
    "dis_merged_corona = interchange_corona(dis_merged)\n",
    "\n",
    "# interchange wuhan and hubei terms\n",
    "dis_merged_corona_wuhan = interchange_wuhan_hubei(dis_merged_corona)\n",
    "\n",
    "# create variant terms among disease, disorder, syndrome, infection and pneumonia\n",
    "dis_merged_corona_wuhan_var = create_variant_terms_dis(dis_merged_corona_wuhan)\n",
    "\n",
    "# remove duplicate words and terms with both new and novel\n",
    "dis_merged_corona_wuhan_var_nodup = remove_duplicates(dis_merged_corona_wuhan_var)\n",
    "\n",
    "# write file\n",
    "write_output_file(dis_out, dis_merged_corona_wuhan_var_nodup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production of Silver Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_lookup import Entity\n",
    "import pandas as pd\n",
    "\n",
    "sci_md = 'path/to/scispacy/en_core_sci_md/en_core_sci_md-0.2.4'\n",
    "\n",
    "# abstract collection\n",
    "abstract = open('path/to/abstracts/gold_abstracts.txt', encoding=\"utf8\").read()\n",
    "abstract = abstract.replace('-', '')\n",
    "abstract = abstract.replace('/', ' ')\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for virus terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load virus dictionary\n",
    "virusterms = []\n",
    "with open(\"virus/output/file.txt\") as viruslist:\n",
    "    for line in viruslist:\n",
    "        line = line.strip()\n",
    "        virusterms.append(line)\n",
    "\n",
    "# create pipeline\n",
    "nlp1 = spacy.load(sci_md, disable = ['ner']) #disabling ner required for Entity to work\n",
    "entity1 = Entity(keywords_list=virusterms,case_sensitive=False) #this merges tokens to match the keywords_list\n",
    "nlp1.add_pipe(entity1, last=True)\n",
    "print(nlp1.pipeline)\n",
    "\n",
    "# create doc\n",
    "doc1 = nlp1(abstract)\n",
    "\n",
    "# add IOB tags\n",
    "df_virus = pd.DataFrame()\n",
    "for token in doc1:\n",
    "    if token._.is_entity:\n",
    "        token_text = token.text\n",
    "        token_text = token.text.split()\n",
    "        if len(token_text) == 1:\n",
    "            token_series = {'token' : token.text, 'label' : \"B\", 'category' : 'Virus_SARS-CoV-2', 'sent': token.sent}\n",
    "            df_virus = df_virus.append(token_series, ignore_index=True)\n",
    "        else:\n",
    "            token_series = {'token' : token_text[0], 'label' : \"B\", 'category' : 'Virus_SARS-CoV-2', 'sent': token.sent}\n",
    "            df_virus = df_virus.append(token_series, ignore_index=True)\n",
    "            index = 1\n",
    "            while index <= (len(token_text)-1):\n",
    "                token_series = {'token' : token_text[index], 'label' : \"I\", 'category' : 'Virus_SARS-CoV-2', 'sent': token.sent,}\n",
    "                df_virus = df_virus.append(token_series, ignore_index=True)\n",
    "                index = index+1\n",
    "    else:\n",
    "        token_series = {'token' : token.text, 'label' : \"O\", 'category' : 'NaN', 'sent': token.sent}\n",
    "        df_virus = df_virus.append(token_series, ignore_index=True)\n",
    "df_virus = df_virus[['token', 'label', 'category', 'sent']]\n",
    "\n",
    "# write file\n",
    "df_virus.to_csv (r'path/to/output_file_virus.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for disease terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseaseterms = []\n",
    "with open(\"disease/output/file.txt\") as diseaselist:\n",
    "    for line in diseaselist:\n",
    "        line = line.strip()\n",
    "        diseaseterms.append(line)\n",
    "\n",
    "# create pipeline\n",
    "nlp2 = spacy.load(sci_md, disable = ['ner']) #disabling ner required for Entity to work\n",
    "entity2 = Entity(keywords_list=diseaseterms,case_sensitive=False) #this merges tokens to match the keywords_list\n",
    "nlp2.add_pipe(entity2, last=True)\n",
    "print(nlp2.pipeline)\n",
    "\n",
    "# create doc\n",
    "doc2 = nlp2(abstract)\n",
    "\n",
    "df_disease = pd.DataFrame()\n",
    "\n",
    "# add IOB tags\n",
    "for token in doc2:\n",
    "    if token._.is_entity:\n",
    "        token_text = token.text\n",
    "        token_text = token.text.split()\n",
    "        if len(token_text) == 1:\n",
    "            token_series = {'token' : token.text, 'label' : \"B\", 'category' : 'Disease_COVID-19', 'sent': token.sent}\n",
    "            df_disease = df_disease.append(token_series, ignore_index=True)\n",
    "        else:\n",
    "            token_series = {'token' : token_text[0], 'label' : \"B\", 'category' : 'Disease_COVID-19', 'sent': token.sent}\n",
    "            df_disease = df_disease.append(token_series, ignore_index=True)\n",
    "            index = 1\n",
    "            while index <= (len(token_text)-1):\n",
    "                token_series = {'token' : token_text[index], 'label' : \"I\", 'category' : 'Disease_COVID-19', 'sent': token.sent,}\n",
    "                df_disease = df_disease.append(token_series, ignore_index=True)\n",
    "                index = index+1\n",
    "    else:\n",
    "        token_series = {'token' : token.text, 'label' : \"O\", 'category' : 'NaN', 'sent': token.sent}\n",
    "        df_disease = df_disease.append(token_series, ignore_index=True)\n",
    "df_disease = df_disease[['token', 'label', 'category', 'sent']]\n",
    "\n",
    "# write file\n",
    "df_disease.to_csv (r'path/to/output_file_disease.csv', index = True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
